# âš¡ Before / After Copilot Example Set

This document illustrates how GitHub Copilot accelerates automation engineering tasks â€” turning hours into minutes â€” while maintaining human oversight and code quality.

Each example includes:
- **Scenario** â€“ the real-world task
- **Before Copilot** â€“ traditional workflow
- **After Copilot** â€“ Copilot-assisted workflow with prompt and result
- **Impact** â€“ measurable improvement

---

## ðŸ§© Example 1: Bash Automation Script

### Scenario
Create a Bash script to check whether multiple services are running and restart any that are down.

### Before Copilot
Manually writing the loop logic, service array, and restart handling:
```bash
#!/usr/bin/env bash
services=("nginx" "postgres" "redis")

for svc in "${services[@]}"; do
  if ! systemctl is-active --quiet "$svc"; then
    echo "Restarting $svc"
    systemctl restart "$svc"
  fi
done
```

### After Copilot

**Prompt:**

> Write a Bash script that checks the status of nginx, postgres, and redis every 5 minutes, restarting any that are down, and log the results to /var/log/service_check.log.

**Copilot Output (refined):**

```bash
#!/usr/bin/env bash
services=(nginx postgres redis)
log="/var/log/service_check.log"

while true; do
  for svc in "${services[@]}"; do
    if ! systemctl is-active --quiet "$svc"; then
      echo "$(date): Restarting $svc" | tee -a "$log"
      systemctl restart "$svc"
    else
      echo "$(date): $svc is healthy" >> "$log"
    fi
  done
  sleep 300
done
```


### Impact

* Time: ~3 minutes
* Copilot added logging and scheduling automatically.
* You refined it by ensuring safe permissions and log rotation.

---

## ðŸ§± Example 2: Ansible Playbook Generation

### Scenario

Deploy PostgreSQL 16 on RHEL9, ensure it starts on boot, and creates a database called `metrics`.

### Before Copilot

Hand-built playbook, using old snippets and searching docs:


```yaml
- name: Install PostgreSQL
  hosts: db
  become: yes
  tasks:
    - name: Install repo
      yum:
        name: https://download.postgresql.org/pub/repos/yum/reporpms/EL-9-x86_64/pgdg-redhat-repo-latest.noarch.rpm
        state: present
    - name: Install PostgreSQL
      yum:
        name: postgresql16-server
        state: present
    - name: Init and enable service
      shell: /usr/pgsql-16/bin/postgresql-16-setup initdb
      args:
        creates: /var/lib/pgsql/16/data/PG_VERSION
    - name: Enable service
      systemd:
        name: postgresql-16
        state: started
        enabled: yes
```

Time: ~45 minutes including YAML debugging and docs lookup.

### After Copilot

**Prompt:**

> Generate an Ansible playbook that installs PostgreSQL 16 on RHEL9, initializes the database, starts and enables the service, and creates a database named metrics.

**Copilot Output (validated):**

```yaml
- name: Deploy PostgreSQL 16
  hosts: db
  become: yes
  tasks:
    - name: Install PostgreSQL repo
      yum:
        name: https://download.postgresql.org/pub/repos/yum/reporpms/EL-9-x86_64/pgdg-redhat-repo-latest.noarch.rpm
        state: present

    - name: Install PostgreSQL
      yum:
        name:
          - postgresql16
          - postgresql16-server
        state: present

    - name: Initialize database
      command: /usr/pgsql-16/bin/postgresql-16-setup initdb
      args:
        creates: /var/lib/pgsql/16/data/PG_VERSION

    - name: Enable and start service
      systemd:
        name: postgresql-16
        state: started
        enabled: yes

    - name: Create metrics database
      become_user: postgres
      postgresql_db:
        name: metrics
```

### Impact

* Time: ~10 minutes total
* Copilot produced correct YAML syntax and order; you reviewed and adjusted minor paths.
* Consistent structure and idempotency preserved.

---

## ðŸ Example 3: Python Data Transformation Script

### Scenario

Convert a CSV file to JSON, grouping entries by a specific column.

### Before Copilot

Manually written:

```python
import csv, json
from collections import defaultdict

data = defaultdict(list)
with open('data.csv') as f:
    reader = csv.DictReader(f)
    for row in reader:
        key = row['category']
        data[key].append(row)

with open('data.json', 'w') as f:
    json.dump(data, f, indent=2)
```

### After Copilot

**Prompt:**

> Write a Python script that reads a CSV file and converts it to a JSON object grouped by the "category" column, saving the output to data.json with proper indentation and UTF-8 encoding.

**Copilot Output:**

```python
import csv, json
from collections import defaultdict
from pathlib import Path

src = Path("data.csv")
out = Path("data.json")
grouped = defaultdict(list)

with src.open(encoding="utf-8") as f:
    for row in csv.DictReader(f):
        grouped[row["category"]].append(row)

with out.open("w", encoding="utf-8") as f:
    json.dump(grouped, f, indent=2, ensure_ascii=False)
```

### Impact

* Time: 3 minutes vs 20 minutes
* Copilot added modern file handling (`Pathlib`) and UTF-8 safety.
* Human validation ensured key naming consistency.

---

## ðŸ“ˆ Summary Metrics

| Task Type            | Manual Effort | Copilot Effort | Improvement | Notes                             |
| -------------------- | ------------- | -------------- | ----------- | --------------------------------- |
| Bash service monitor | 15 min        | 3 min          | 80%         | Added logging automatically       |
| Ansible playbook     | 45 min        | 10 min         | 78%         | Generated correct YAML + handlers |
| Python CSVâ†’JSON      | 20 min        | 3 min          | 85%         | Added modern best practices       |

---

## ðŸ’¡ Key Takeaways

* Copilot accelerates *routine engineering work* dramatically when prompts are structured and reviewed.
* Effective use depends on **clear intent** and **safe oversight**.
* The best productivity gains appear in repetitive scripting, boilerplate, and refactoring scenarios.

> *Human judgment + Copilot speed = sustainable velocity.*
